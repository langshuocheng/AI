{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f610851",
   "metadata": {},
   "source": [
    "## Chinese BERT with Whole Word Masking\n",
    "For further accelerating Chinese natural language processing, we provide **Chinese pre-trained BERT with Whole Word Masking**. \n",
    "\n",
    "**[Pre-Training with Whole Word Masking for Chinese BERT](https://arxiv.org/abs/1906.08101)**  \n",
    "Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu\n",
    "\n",
    "This repository is developed based on：https://github.com/google-research/bert\n",
    "\n",
    "You may also interested in,\n",
    "- Chinese BERT series: https://github.com/ymcui/Chinese-BERT-wwm\n",
    "- Chinese MacBERT: https://github.com/ymcui/MacBERT\n",
    "- Chinese ELECTRA: https://github.com/ymcui/Chinese-ELECTRA\n",
    "- Chinese XLNet: https://github.com/ymcui/Chinese-XLNet\n",
    "- Knowledge Distillation Toolkit - TextBrewer: https://github.com/airaria/TextBrewer\n",
    "\n",
    "More resources by HFL: https://github.com/ymcui/HFL-Anthology\n",
    "\n",
    "## 示例代码\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04539e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "\n",
    "pipeline_ins = pipeline(\n",
    "\t\t'fill-mask',\n",
    "\t\tmodel='dienstag/chinese-bert-wwm-ext',\n",
    "        model_revision='v1.0.0'\n",
    ")\n",
    "\n",
    "print(pipeline_ins('巴黎是[MASK]国的首都。'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd6a2f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Citation\n",
    "If you find the technical report or resource is useful, please cite the following technical report in your paper.\n",
    "- Primary: https://arxiv.org/abs/2004.13922\n",
    "```\n",
    "@inproceedings{cui-etal-2020-revisiting,\n",
    "    title = \"Revisiting Pre-Trained Models for {C}hinese Natural Language Processing\",\n",
    "    author = \"Cui, Yiming  and\n",
    "      Che, Wanxiang  and\n",
    "      Liu, Ting  and\n",
    "      Qin, Bing  and\n",
    "      Wang, Shijin  and\n",
    "      Hu, Guoping\",\n",
    "    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings\",\n",
    "    month = nov,\n",
    "    year = \"2020\",\n",
    "    address = \"Online\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://www.aclweb.org/anthology/2020.findings-emnlp.58\",\n",
    "    pages = \"657--668\",\n",
    "}\n",
    "```\n",
    "- Secondary: https://arxiv.org/abs/1906.08101  \n",
    "```\n",
    "@article{chinese-bert-wwm,\n",
    "  title={Pre-Training with Whole Word Masking for Chinese BERT},\n",
    "  author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing and Wang, Shijin and Hu, Guoping},\n",
    "  journal={arXiv preprint arXiv:1906.08101},\n",
    "  year={2019}\n",
    " }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76fcaa44-feb2-4db9-8443-6bff2509f7de",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-01-22T03:15:16.773983Z",
     "iopub.status.busy": "2024-01-22T03:15:16.773623Z",
     "iopub.status.idle": "2024-01-22T03:15:16.794320Z",
     "shell.execute_reply": "2024-01-22T03:15:16.793797Z",
     "shell.execute_reply.started": "2024-01-22T03:15:16.773963Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 加载Bert预训练模型的分词器\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/workspace/model/chinese_bert\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForSequenceClassification\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/workspace/model/chinese_bert\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 原始文本数据\u001b[39;00m\n\u001b[1;32m     10\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis is an example sentence.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnother example sentence.\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertForSequenceClassification' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 加载Bert预训练模型的分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('/mnt/workspace/model/chinese_bert')\n",
    "\n",
    "# 原始文本数据\n",
    "texts = ['This is an example sentence.', 'Another example sentence.']\n",
    "\n",
    "# 标签\n",
    "labels = ['label11', 'label12']\n",
    "\n",
    "# 创建空列表用于存储标注后的数据\n",
    "labeled_data = []\n",
    "\n",
    "# 对每个文本进行标注\n",
    "for text, label in zip(texts, labels):\n",
    "    # 使用Bert分词器对文本进行分词\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # 将分词后的文本转换为Bert模型所需的输入格式\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = tokenizer.build_inputs_with_special_tokens(input_ids)\n",
    "    \n",
    "    # 创建样本字典，包含输入文本和标签\n",
    "    sample = {'input_ids': input_ids, 'label': label}\n",
    "    \n",
    "    # 将标注后的样本添加到列表中\n",
    "    labeled_data.append(sample)\n",
    "\n",
    "# 打印标注后的数据\n",
    "for sample in labeled_data:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb64441a-ce83-4f6e-8726-c21db32d64a2",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-01-22T03:38:26.216296Z",
     "iopub.status.busy": "2024-01-22T03:38:26.215975Z",
     "iopub.status.idle": "2024-01-22T03:38:26.220824Z",
     "shell.execute_reply": "2024-01-22T03:38:26.220331Z",
     "shell.execute_reply.started": "2024-01-22T03:38:26.216278Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 151, 10295, 8154, 8554, 11099, 106, 102], 'label': 'positive'}\n",
      "{'input_ids': [101, 8554, 9106, 8310, 8413, 8139, 10112, 8291, 106, 102], 'label': 'positive'}\n",
      "{'input_ids': [101, 8174, 8997, 12290, 8310, 10192, 11262, 119, 102], 'label': 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 原始文本数据\n",
    "texts = ['I hite this movie!', 'This book is amazing!', 'The weather is nice today.']\n",
    "\n",
    "# 标签\n",
    "labels = ['positive', 'positive', 'neutral']\n",
    "\n",
    "# 创建空列表用于存储标注后的数据\n",
    "labeled_data = []\n",
    "\n",
    "# 对每个文本进行标注\n",
    "for text, label in zip(texts, labels):\n",
    "    # 使用Bert分词器对文本进行分词\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    \n",
    "    # 将分词后的文本转换为Bert模型所需的输入格式\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = tokenizer.build_inputs_with_special_tokens(input_ids)\n",
    "    \n",
    "    # 创建样本字典，包含输入文本和标签\n",
    "    sample = {'input_ids': input_ids, 'label': label}\n",
    "    \n",
    "    # 将标注后的样本添加到列表中\n",
    "    labeled_data.append(sample)\n",
    "\n",
    "# 打印标注后的数据\n",
    "for sample in labeled_data:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d764e-ca77-41f3-a6c3-244f7e8a7b54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-22T03:38:28.906421Z",
     "iopub.status.busy": "2024-01-22T03:38:28.906102Z",
     "iopub.status.idle": "2024-01-22T03:38:28.910936Z",
     "shell.execute_reply": "2024-01-22T03:38:28.910394Z",
     "shell.execute_reply.started": "2024-01-22T03:38:28.906402Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [101, 8701, 117, 9510, 8995, 8357, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Token Type IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 输入文本\n",
    "text = \"Hello, how are you?\"\n",
    "\n",
    "# 使用tokenizer.encode_plus方法进行编码\n",
    "encoding = tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens=True,  # 添加特殊标记\n",
    "    max_length=512,  # 设置最大长度\n",
    "    padding=\"max_length\",  # 填充序列至最大长度\n",
    "    truncation=True,  # 截断序列\n",
    "    return_attention_mask=True,  # 返回注意力掩码\n",
    "    return_token_type_ids=True  # 返回段落编号\n",
    ")\n",
    "\n",
    "input_ids = encoding[\"input_ids\"]\n",
    "attention_mask = encoding[\"attention_mask\"]\n",
    "token_type_ids = encoding[\"token_type_ids\"]\n",
    "\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention Mask:\", attention_mask)\n",
    "print(\"Token Type IDs:\", token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b24a7-af21-48fa-8e94-c286d4a1ddb8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AutoModel\n",
    "\n",
    "# 初始化Bert分词器和Bert模型\n",
    "MODEL_DIR = os.path.abspath(\"/\")\n",
    "model_dir = os.path.join(MODEL_DIR, '/mnt/workspace/model/chinese_L-12_H-768_A-12')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('/mnt/workspace/model/chinese_L-12_H-768_A-12')\n",
    "bert_config = BertConfig.from_pretrained(model_dir)\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir, config=bert_config)\n",
    "\n",
    "# 输入文本和标签\n",
    "texts = [\"I love this movie\", \"This is a great book\", \"I hate this product\", \"The weather is nice today\"]\n",
    "labels = [1, 1, 0, 1]\n",
    "\n",
    "# 编码文本并进行文本分类\n",
    "for text, label in zip(texts, labels):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,  # 添加特殊标记\n",
    "        max_length=512,  # 设置最大长度\n",
    "        padding=\"max_length\",  # 填充序列至最大长度\n",
    "        truncation=True,  # 截断序列\n",
    "        return_attention_mask=True,  # 返回注意力掩码\n",
    "        return_token_type_ids=True  # 返回段落编号\n",
    "    )\n",
    "\n",
    "    input_ids = torch.tensor([encoding[\"input_ids\"]])\n",
    "    attention_mask = torch.tensor([encoding[\"attention_mask\"]])\n",
    "    token_type_ids = torch.tensor([encoding[\"token_type_ids\"]])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    predicted_label = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "    print(\"Text:\", text)\n",
    "    print(\"True Label:\", label)\n",
    "    print(\"Predicted Label:\", predicted_label)\n",
    "    print(\"Probabilities:\", probabilities)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca55c3-4959-4204-be7d-b027bf4e8bce",
   "metadata": {},
   "source": [
    "transformers-cli convert --model_type bert --tf_checkpoint ./chinese_L-12_H-768_A-12/bert_model.ckpt --config ./chinese_L-12_H-768_A-12/config.json --pytorch_dump_output ./chinese_L-12_H-768_A-12/pytorch_model.bin\n",
    "\n",
    "转换\n",
    "\n",
    "wget https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "110f2140-a98e-40dc-afb5-c532da15f8ed",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-01-22T06:15:06.412444Z",
     "iopub.status.busy": "2024-01-22T06:15:06.412090Z",
     "iopub.status.idle": "2024-01-22T06:15:06.417123Z",
     "shell.execute_reply": "2024-01-22T06:15:06.416656Z",
     "shell.execute_reply.started": "2024-01-22T06:15:06.412414Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11.])\n"
     ]
    }
   ],
   "source": [
    "# python\n",
    "import torch\n",
    "\n",
    "# 创建一个需要计算梯度的张量\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# 定义一个函数\n",
    "def f(x):\n",
    "    return 2 * x**2 + 3 * x - 4\n",
    "\n",
    "# 计算函数在 x 处的值\n",
    "y = f(x)\n",
    "\n",
    "# 计算梯度\n",
    "y.backward()\n",
    "\n",
    "# 获取梯度值\n",
    "gradient = x.grad\n",
    "\n",
    "print(gradient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
